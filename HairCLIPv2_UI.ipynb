{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JgwuZJuIziYd",
        "u_cdz8cVzqf5",
        "Mq8PTfK6z4et"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone & Install"
      ],
      "metadata": {
        "id": "JgwuZJuIziYd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAxGEefpsyAX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/C0untFloyd/HairCLIPv2_UI.git\n",
        "%cd HairCLIPv2_UI\n",
        "# On Colab everything Torch should be pre-installed already\n",
        "# !pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ftfy regex tqdm matplotlib jupyter ipykernel opencv-python scikit-image kornia==0.6.7 face-alignment==1.3.5 dlib==19.22.1\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init"
      ],
      "metadata": {
        "id": "kEZ2Gkaw0Tgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from scripts.Embedding import Embedding\n",
        "from scripts.text_proxy import TextProxy\n",
        "from scripts.ref_proxy import RefProxy\n",
        "from scripts.sketch_proxy import SketchProxy\n",
        "from scripts.bald_proxy import BaldProxy\n",
        "from scripts.color_proxy import ColorProxy\n",
        "from scripts.feature_blending import hairstyle_feature_blending\n",
        "from utils.seg_utils import vis_seg\n",
        "from utils.mask_ui import painting_mask\n",
        "from utils.image_utils import display_image_list, process_display_input\n",
        "from utils.model_utils import load_base_models\n",
        "from utils.options import Options\n",
        "\n",
        "opts = Options().parse(jupyter=True)\n",
        "src_name = '168125'# source image name you want to edit\n",
        "\n",
        "image_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "g_ema, mean_latent_code, seg = load_base_models(opts)\n",
        "ii2s = Embedding(opts, g_ema, mean_latent_code[0,0])\n",
        "if not os.path.isfile(os.path.join(opts.src_latent_dir, f\"{src_name}.npz\")):\n",
        "    inverted_latent_w_plus, inverted_latent_F = ii2s.invert_image_in_FS(image_path=f'{opts.src_img_dir}/{src_name}.jpg')\n",
        "    save_latent_path = os.path.join(opts.src_latent_dir, f'{src_name}.npz')\n",
        "    np.savez(save_latent_path, latent_in=inverted_latent_w_plus.detach().cpu().numpy(),\n",
        "                latent_F=inverted_latent_F.detach().cpu().numpy())\n",
        "src_latent = torch.from_numpy(np.load(f'{opts.src_latent_dir}/{src_name}.npz')['latent_in']).cuda()\n",
        "src_feature = torch.from_numpy(np.load(f'{opts.src_latent_dir}/{src_name}.npz')['latent_F']).cuda()\n",
        "src_image = image_transform(Image.open(f'{opts.src_img_dir}/{src_name}.jpg').convert('RGB')).unsqueeze(0).cuda()\n",
        "input_mask = torch.argmax(seg(src_image)[1], dim=1).long().clone().detach()\n",
        "\n",
        "bald_proxy = BaldProxy(g_ema, opts.bald_path)\n",
        "text_proxy = TextProxy(opts, g_ema, seg, mean_latent_code)\n",
        "ref_proxy = RefProxy(opts, g_ema, seg, ii2s)\n",
        "sketch_proxy = SketchProxy(g_ema, mean_latent_code, opts.sketch_path)\n",
        "color_proxy = ColorProxy(opts, g_ema, seg)\n",
        "\n",
        "edited_hairstyle_img = src_image\n",
        "def hairstyle_editing(global_cond=None, local_sketch=False, paint_the_mask=False, \\\n",
        "                      src_latent=src_latent, src_feature=src_feature, input_mask=input_mask, src_image=src_image, \\\n",
        "                        latent_global=None, latent_local=None, latent_bald=None, local_blending_mask=None, painted_mask=None):\n",
        "    if paint_the_mask:\n",
        "        modified_mask = painting_mask(input_mask)\n",
        "        input_mask = torch.from_numpy(modified_mask).unsqueeze(0).cuda().long().clone().detach()\n",
        "        vis_modified_mask = vis_seg(modified_mask)\n",
        "        display_image_list([src_image, vis_modified_mask])\n",
        "        painted_mask = input_mask\n",
        "\n",
        "    if local_sketch:\n",
        "        latent_local, local_blending_mask, visual_local_list = sketch_proxy(input_mask)\n",
        "        display_image_list(visual_local_list)\n",
        "\n",
        "    if global_cond is not None:\n",
        "        assert isinstance(global_cond, str)\n",
        "        latent_bald, visual_bald_list = bald_proxy(src_latent)\n",
        "        display_image_list(visual_bald_list)\n",
        "\n",
        "        if global_cond.endswith('.jpg') or global_cond.endswith('.png'):\n",
        "            latent_global, visual_global_list = ref_proxy(global_cond, src_image, painted_mask=painted_mask)\n",
        "        else:\n",
        "            latent_global, visual_global_list = text_proxy(global_cond, src_image, from_mean=True, painted_mask=painted_mask)\n",
        "        display_image_list(visual_global_list)\n",
        "\n",
        "    src_feature, edited_hairstyle_img = hairstyle_feature_blending(g_ema, seg, src_latent, src_feature, input_mask, latent_bald=latent_bald,\\\n",
        "                                                latent_global=latent_global, latent_local=latent_local, local_blending_mask=local_blending_mask)\n",
        "    return src_feature, edited_hairstyle_img\n"
      ],
      "metadata": {
        "id": "Ed5mQ8nPung2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hairstyle Edit"
      ],
      "metadata": {
        "id": "u_cdz8cVzqf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#global_cond: e.g. 'bowl cut hairstyle' for text_mode; '058728.jpg' for ref_mode\n",
        "src_feature, edited_hairstyle_img = hairstyle_editing(global_cond='bowl cut hairstyle', local_sketch=False, paint_the_mask=False)\n",
        "display_image_list([process_display_input(src_image), process_display_input(edited_hairstyle_img)])"
      ],
      "metadata": {
        "id": "uj2T1UVOxxUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Color Edit"
      ],
      "metadata": {
        "id": "Mq8PTfK6z4et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_cond = '108157.jpg' #e.g. 'red hair' for text_mode; '108157.jpg' for ref_mode; (220,220,220) for RGB value mode\n",
        "\n",
        "visual_color_list, visual_final_list = color_proxy(color_cond, edited_hairstyle_img, src_latent, src_feature)\n",
        "display_image_list(visual_color_list)\n",
        "display_image_list(visual_final_list)"
      ],
      "metadata": {
        "id": "wjFUvoEA0AfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}